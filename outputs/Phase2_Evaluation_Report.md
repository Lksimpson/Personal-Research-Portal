# Phase 2 Evaluation Report: RAG System for Remote Work Productivity Research

**Author:** Lindberg Simpson  
**Date:** February 15, 2026  
**Course:** AI Model Development, CMU Heinz AIM  
**System:** Research Portal RAG Pipeline

---

## Executive Summary

This report evaluates the Phase 2 implementation of a Retrieval-Augmented Generation (RAG) system designed to answer research questions about remote work and productivity using a curated corpus of 15 academic and industry sources. The system implements a complete end-to-end pipeline including document ingestion, semantic chunking, vector-based retrieval, and answer generation with source citations. Phase 2 introduces three key enhancements: automated evaluation metrics (groundedness and answer relevance computed via sentence-matching and keyword-overlap algorithms), structured citations (inline source references combined with formatted bibliographic lists from the data manifest), and evidence strength scoring (High/Medium/Low confidence labels based on retrieval similarity thresholds). The evaluation uses a test set of 20 queries across three categories (direct factual, synthesis, and edge cases) to assess system performance. Results show strong retrieval performance (90% top-5 precision) and consistent citation behavior, with automated metrics revealing moderate groundedness (42% sentence-level overlap) and high answer relevance (87% keyword overlap). The system demonstrates feasibility for exploratory research support but exhibits hallucination patterns typical of local language models that require human verification.

---

## 1. System Architecture and Implementation

### 1.1 Pipeline Overview

The implemented RAG system follows a modular architecture across five core stages: document ingestion and parsing, semantic chunking, embedding and indexing, retrieval, and answer generation. Document ingestion uses PyPDF to extract text from 15 PDF sources covering remote work productivity research, including working papers (SRC001, SRC002, SRC003, SRC011, SRC012), peer-reviewed studies (SRC004, SRC008, SRC009, SRC010, SRC013, SRC014, SRC015), government reports (SRC005), and industry surveys (SRC006, SRC007). Each source is parsed into structured JSON with metadata including title, authors, year, and source type.

The chunking strategy divides documents into approximately 2048-character segments with 256-character overlap, respecting paragraph boundaries to preserve semantic coherence. This produced a corpus of 191 chunks indexed using FAISS with dense embeddings generated by the sentence-transformers model (all-MiniLM-L6-v2, 384 dimensions). The pipeline supports multiple embedding backends including local sentence-transformers, FastEmbed (ONNX-based), and API-based options (OpenAI, Gemini) with automatic fallback logic.

Answer generation employs a local language model (Ollama with gemma3:4b) following API quota exhaustion with cloud providers. The generation step receives the top-k retrieved chunks and a system prompt requiring citation-backed answers with explicit acknowledgment of missing evidence. 

Phase 2 enhancements augment the baseline RAG pipeline:

1. **Structured Citations** (src/rag/citations.py): Each answer includes inline (source_id, chunk_id) citations for claim-level verification, plus an automatically generated "References" section with formatted bibliographic entries (Authors, Year, Title, URL/DOI) pulled from data_manifest.csv.

2. **Evidence Strength Scoring** (src/rag/citations.py): Answers display an "Evidence strength" section categorizing retrieved chunks as High (≥0.45 similarity), Medium (≥0.30), or Low (<0.30) based on retrieval cosine similarity scores, providing users with confidence calibration.

3. **Automated Evaluation Metrics** (src/eval/metrics.py): The evaluation pipeline computes groundedness (fraction of answer sentences with direct textual overlap in retrieved chunks) and answer_relevance (fraction of query keywords present in answer) for every query, enabling systematic quality tracking without manual scoring.

All queries, retrieved chunks with similarity scores, generated answers (with citations and evidence strength), evaluation metrics, and metadata are logged to JSONL format for reproducibility and analysis.

### 1.2 Design Decisions and Trade-offs

Several design decisions shaped system behavior and performance. The chunk size of 2048 characters balances retrieval granularity with semantic completeness, though some complex tables and multi-part findings span multiple chunks, requiring synthesis across retrievals. The 256-character overlap mitigates boundary effects but increases index size by approximately 15%. The choice of all-MiniLM-L6-v2 for embeddings prioritizes speed and local deployment over the higher semantic quality of larger models, which is evident in some retrieval mismatches for nuanced queries.

The system prompt emphasizes groundedness and explicit citation format (source_id, chunk_id), which successfully constrains fabrication but also results in verbose, citation-heavy responses that sometimes obscure the direct answer. The decision to use a local LLM (gemma3:4b) after Gemini quota limits introduced a capability-vs-cost trade-off: local generation avoids external dependencies but exhibits lower instruction-following fidelity, more verbose output, and occasional hallucination of specific numbers not present in retrieved context.

The evaluation query set of 20 queries was designed with explicit expected behaviors across three difficulty tiers: 10 direct factual queries requiring single-source extraction (e.g., specific statistics, sample sizes, metrics), 5 synthesis queries requiring multi-source integration and comparison (e.g., mechanism synthesis, cross-study agreement), and 5 edge cases testing uncertainty handling and classification (e.g., absence of evidence, measurement type categorization). The test set reflects realistic use cases for the research domain and covers the major sources in the corpus.

---

## 2. Evaluation Methodology and Metrics

### 2.1 Evaluation Design

The system was evaluated on 20 hand-crafted queries with known expected behaviors documented in queries.jsonl. Each query specifies the target information, expected sources, and evaluation focus (e.g., numeric extraction, multi-source synthesis, absence-of-evidence handling). For each query, the system retrieved the top-5 chunks by cosine similarity and generated an answer using the local LLM, with all outputs logged to outputs/eval_runs/eval_run10.jsonl.

Two primary metrics were calculated automatically for each query using custom evaluation functions: groundedness (the fraction of answer sentences that textually overlap with retrieved chunk content) and answer relevance (the fraction of query keywords present in the answer). These metrics are implemented in src/eval/metrics.py and computed during the evaluation pipeline execution. Groundedness uses sentence-level string matching to verify whether answer statements appear in the retrieved context, while answer relevance performs keyword-based overlap analysis between query and answer text. Both metrics operate on a continuous 0-1 scale.

Additional qualitative analysis examined citation accuracy (whether cited source_ids and chunk_ids match retrieved sources), hallucination patterns (fabricated statistics, misattributed claims, or injected references), and uncertainty calibration (whether the system appropriately signals missing evidence or limits of support).

### 2.2 Scoring Criteria and Enhancement Features

Groundedness scoring uses sentence-level text matching: the compute_groundedness() function splits the answer into sentences and checks whether each sentence appears (case-insensitive substring match) in the concatenated text of retrieved chunks. The metric represents the fraction of answer sentences with direct textual support, providing an automated proxy for factual grounding. This approach captures exact or near-exact quote usage but may undercount paraphrased content or reasonable inferences.

Answer relevance scoring uses keyword overlap: the compute_answer_relevance() function extracts words from both query and answer, then calculates the fraction of query keywords present in the answer. This metric captures whether the answer addresses the query's topic and terminology, penalizing off-topic content and rewarding direct addressing of query terms. However, it does not assess answer completeness or correctness.

The system implements three key enhancements beyond basic RAG:

1. **Evidence Strength Scoring**: Each answer includes an automatically generated "Evidence strength" section that categorizes retrieved chunks by retrieval confidence. Chunks with cosine similarity ≥0.45 are labeled "High," ≥0.30 as "Medium," and <0.30 as "Low." This allows users to gauge the reliability of evidence supporting the answer and identifies when retrieval quality is marginal.

2. **Structured Citations**: Answers combine inline (source_id, chunk_id) citations with an automatically generated "References" section. The format_reference_list() function queries data_manifest.csv to construct formatted bibliographic entries (Authors, Year, Title, URL/DOI) for all cited sources, enabling proper academic attribution and verification.

3. **Automated Evaluation Metrics**: The evaluation pipeline computes groundedness and answer_relevance automatically for each query during the eval run, logging results to JSONL. This eliminates manual scoring overhead and enables scalable, reproducible assessment across system iterations.

Citation accuracy was assessed qualitatively: an answer received credit for correct citation format and source matching only if all cited (source_id, chunk_id) pairs appeared in the top-5 retrieved set. Hallucinated references (e.g., fabricated URLs, non-existent table numbers, or citations to sources outside the corpus) were flagged as failures. The report also tracked retrieval precision (whether the top-5 chunks contained the target information) and retrieval diversity (whether multiple relevant sources were surfaced when appropriate).

---

## 3. Results and Findings

### 3.1 Quantitative Performance

Across the 20-query evaluation set, the system achieved a mean automated groundedness score of 0.42 (range: 0.29–0.52) and a mean automated answer relevance score of 0.87 (range: 0.75–1.0). These metrics were computed automatically by the evaluation pipeline using sentence-matching and keyword-overlap algorithms implemented in src/eval/metrics.py. Direct factual queries (Q01–Q10) showed higher answer relevance (mean 0.88) but moderate groundedness (mean 0.44), reflecting the system's tendency to provide correct general information with some unsupported details. Synthesis queries (Q11–Q15) exhibited similar groundedness (mean 0.41) and slightly lower relevance (mean 0.84), as answers sometimes strayed into tangential discussion. Edge case queries (Q16–Q20) achieved the lowest groundedness (mean 0.38) and relevance (mean 0.86), indicating challenges with uncertainty handling and complex classifications.

Retrieval performance was consistently strong, with the top-5 chunks containing at least one relevant source for 18 of 20 queries (90% precision at k=5). The top-ranked chunk was directly relevant for 14 queries (70%), and the retrieval set surfaced the expected primary source in the top-3 for 16 queries (80%). The evidence strength scoring enhancement provides visibility into this retrieval quality: High-confidence chunks (≥0.45 similarity) appeared in 85% of answers, Medium-confidence chunks in 95%, and purely Low-confidence retrievals (<0.30) in only 2 queries, indicating generally robust semantic matching. However, retrieval diversity was limited: for synthesis queries requiring multiple sources, the top-5 often favored a single dominant source (e.g., SRC012, SRC013) due to high overlap in embedding space, reducing coverage of contrasting evidence.

Citation behavior was disciplined: 19 of 20 answers used the required (source_id, chunk_id) format, and 17 answers cited only sources present in the retrieved top-5. The structured citations enhancement successfully appended formatted References sections to all answers, with 100% of answers including properly formatted bibliographic entries from data_manifest.csv. However, within-answer citation accuracy was uneven—answers frequently attributed specific claims to general chunks or cited the same chunk for multiple distinct claims, reducing citation granularity. Two answers (Q07, Q09) fabricated specific table references (e.g., "Table A3") that did not exist in the retrieved context, indicating hallucination of structural details.

### 3.2 Qualitative Observations

The system demonstrated strong adherence to the instruction to avoid fabricating evidence when no support exists, though the execution varied by query complexity. For direct factual queries where the corpus lacked the requested metric (e.g., Q01 asking for average commute-time savings), the system often acknowledged the limitation (e.g., "the corpus does not provide the exact average") but then proceeded to offer related information or inferred estimates, which undermined the initial honesty. This pattern suggests the local LLM struggles with uncertainty calibration—it correctly identifies missing data but cannot resist providing a partial answer.

Synthesis queries revealed both strengths and weaknesses in multi-source reasoning. For Q11 (comparing measurement approaches in SRC004 vs SRC010), the system successfully retrieved both sources and noted differences in metrics (performance grades vs output measures) but failed to discuss measurement limitations as requested. For Q12 (synthesizing mechanisms for WFH persistence), the answer integrated commute-time savings, learning, and preferences correctly but introduced "employer reoptimization" without citing a specific chunk mentioning that term, indicating inference beyond retrieved context.

Edge case performance highlighted brittleness in classification and absence-of-evidence handling. For Q16 (does corpus show remote work increases promotion rates?), the system correctly stated "not found" but then speculated about which sources "might" contain it, rather than definitively stating absence. For Q17 (classify sources by measurement type), the system attempted categorization but conflated self-reported perceptions with objective measures in some cases and provided source_ids not strongly supported by retrieved chunks.

Hallucination patterns were concentrated in specific types: fabricated numeric details (e.g., percentages not in the text), invented structural references (table numbers, figure citations), and injected non-corpus citations (URLs, DOIs not in the retrieved chunks). The system also exhibited a tendency to generate reference lists with correct-looking formatting and DOI structures but occasionally included sources not present in the corpus or mislabeled chunks, reducing trust in the citations section.

---

## 4. Error Analysis and Failure Modes

### 4.1 Retrieval Errors

Retrieval failures occurred in two primary modes: semantic mismatch and source imbalance. Semantic mismatch was observed for queries using domain-specific terminology not well-represented in the embedding space (e.g., "promotion rates," "industry-level TFP"), where the retriever favored general productivity discussions over narrowly relevant passages. For Q16 (promotion rates), the top-5 chunks discussed performance and retention but did not mention promotions, and the system failed to acknowledge this gap cleanly.

Source imbalance was pronounced for synthesis queries: the retriever often returned 3–4 chunks from a single source (e.g., SRC012 or SRC013) despite multiple sources being tagged as relevant in the data manifest. This occurred because these sources had longer, more detailed discussions that matched query embeddings across multiple chunks, crowding out shorter but equally relevant sources. For Q14 (integrating psychosocial factors across SRC008, SRC015, SRC013), only SRC013 appeared in the top-5, limiting synthesis scope.

A third, less frequent mode was boundary fragmentation: when a critical finding spanned a chunk boundary, retrieval sometimes surfaced only the second half (containing the metric) without the context (containing the study design or scope), leading to ambiguous or incomplete retrieved passages. This was observed for Q03 (defining "effective work time" in SRC010), where the definition was split across two chunks and only one was retrieved.

### 4.2 Generation Errors

Generation errors clustered into four categories: hallucination, citation misattribution, verbosity, and instruction drift. Hallucination most commonly involved specific numbers or structural elements. For Q01, the system stated "the average daily commute-time savings is 1 hour and 20 minutes (60 minutes)," citing SRC001_chunk_0, but the retrieved chunk did not contain that specific 60-minute figure. For Q07, the system referenced "Table A3" and "Table A5" with precise percentages (80.7%, 61.4%) that were not present in the retrieved Cisco study excerpt.

Citation misattribution occurred when the system paired a correct claim with an incorrect or overly general chunk citation. For Q02, the answer discussed a 25% attrition reduction but cited SRC004_chunk_0 broadly rather than pinpointing the sentence containing the statistic. In several cases, the system cited the same chunk for multiple unrelated claims within a single answer, reducing citation usefulness.

Verbosity manifested as lengthy preambles, repeated restatements of the question, or inclusion of tangential domain knowledge not requested by the user. For Q09 (sample size and relationship in Adomako et al.), the answer included a multi-paragraph discussion of general flexible work benefits before eventually mentioning the study, diluting answer relevance. This pattern persisted despite the system prompt emphasizing conciseness.

Instruction drift occurred when the system ignored specific sub-questions or constraints. For Q03 (three outcome variables + definition of effective work time), the answer discussed productivity generally but did not enumerate three specific variables or define "effective work time." For Q08 (databases and keyword groups in systematic review), the answer fabricated a detailed list of PubMed, CINAHL, Scopus, Web of Science, and Google Scholar with specific MeSH terms, despite the query retrieving chunks unrelated to the Ferrara et al. review methods (retrieval failure compounded by hallucination).

### 4.3 Edge Case Robustness

The system struggled most with queries designed to test absence-of-evidence handling and uncertainty calibration. For Q16 (promotion rates), the ideal response would be "No evidence in corpus; promotion rates not measured in these sources." Instead, the system hedged: "does not explicitly state… we can infer it from…" and then speculated, reducing trustworthiness. For Q19 (harm to innovation/collaboration), the system acknowledged limited direct evidence but then provided generic discussion not tied to retrieved chunks.

Classification tasks (Q17: objective vs self-reported measures) revealed confusion: the system listed source_ids in each category but made classification errors (e.g., listing SRC010 as self-reported when it used firm analytics data) and provided weak justification. The answer also included a caveated statement "acknowledge ambiguity if measurement type is mixed," which was appropriate but executed poorly—the system did not clearly explain why specific sources were ambiguous.

Nuanced synthesis (Q18: time reallocation to leisure vs work) required integrating SRC001 (time savings) and SRC012 (hours fall) to show mixed evidence. The system correctly identified both sources but overstated the consensus, claiming "evidence supports" without noting the complexity or methodological differences between time-use surveys and output-per-worker models.

---

## 5. Strengths and Limitations

### 5.1 System Strengths

The system's primary strength is robust retrieval for direct factual queries with clear semantic overlap between query and corpus. For 14 of 20 queries, the top-ranked chunk was directly relevant, and retrieval latency averaged under 200ms per query, enabling real-time research assistance use cases. The chunking strategy successfully preserved paragraph integrity, and retrieved chunks were readable and context-sufficient in most cases.

Citation discipline was consistently enforced: nearly all answers used the required format, and hallucinated citations to external sources were rare (2 of 20 queries). This is a significant achievement given the local LLM's limited training compared to cloud models. The system also demonstrated appropriate caution in several edge cases, explicitly stating when evidence was incomplete or missing, though execution was inconsistent.

Modularity and extensibility are architectural strengths: the pipeline cleanly separates ingestion, chunking, retrieval, and generation, with swappable embedding and LLM backends. The decision to implement local LLM fallback ensured continuous operation despite API quota limits, and the JSONL logging format enables reproducible evaluation and easy post-analysis.

The Phase 2 enhancements demonstrate thoughtful engineering. Evidence strength scoring provides transparency about retrieval confidence without requiring additional models or APIs—it leverages existing similarity scores with intuitive High/Medium/Low bucketing (thresholds: 0.45, 0.30). Structured citations successfully integrate two complementary citation styles: inline (source_id, chunk_id) pairs enable claim-level verification, while the manifest-derived References section provides complete bibliographic information for academic compliance. The automated metrics (groundedness, relevance) enable systematic tracking of answer quality across system iterations and parameter sweeps, transforming evaluation from a one-time manual effort into a continuous monitoring capability.

The evaluation test set itself is a strength—20 queries with documented expected behaviors provide a reusable benchmark for system iteration. The three-tier structure (direct, synthesis, edge) covers realistic use cases and failure modes, and the queries are grounded in the actual corpus contents, avoiding unrealistic expectations.

### 5.2 System Limitations

The dominant limitation is generation quality with the local LLM. Hallucination of specific numbers, table references, and citations reduces trust and requires manual verification of all factual claims. The model's verbosity and tendency to provide tangential discussion lower answer relevance and user efficiency. Instruction-following is inconsistent: the system often acknowledges constraints (e.g., "cite only provided chunks") but then violates them within the same answer.

Retrieval diversity for synthesis queries is a structural limitation: cosine similarity favors dense, detailed sources over concise mentions, causing source imbalance in the top-k. This limits the system's utility for comparative or multi-perspective queries. The current solution (increasing k) trades diversity for noise, as lower-ranked chunks are often off-topic.

Groundedness metrics reveal a systematic gap: only 42% of answer content has direct sentence-level textual overlap with retrieved chunks. This reflects the automated metric's conservative matching approach—it captures direct quotes and near-verbatim restatements but assigns zero credit to paraphrased content or reasonable inferences. The true grounding rate likely lies between this lower bound (42%) and human judgment, which would credit some paraphrasing. However, qualitative analysis confirms substantial hallucination (fabricated statistics, invented table references) beyond metric limitations, indicating genuine generation quality issues. This gap would be unacceptable in high-stakes applications like legal or medical research support.

The corpus itself introduces limitations: 15 sources provide limited coverage, especially for edge cases (e.g., promotion rates, sectoral breakdowns). Several queries (Q16, Q19) expected explicit statements of absence, but the system could not distinguish between "corpus lacks this info" and "retrieval failed to find it," leading to ambiguous responses.

The automated evaluation metrics, while scalable and reproducible, have methodological constraints. The sentence-matching groundedness metric cannot assess paraphrase quality or inference validity, potentially underestimating well-grounded but rephrased content. The keyword-overlap relevance metric cannot detect factual errors or assess answer completeness. More sophisticated metrics (e.g., LLM-as-judge, semantic entailment models) would provide richer quality assessment but were not implemented in Phase 2 due to computational and complexity constraints.

---

## 6. Discussion and Recommendations

### 6.1 Comparison to Phase 1 Goals

Phase 1 established three system requirements: accurately retrieve relevant research evidence, generate citation-backed answers, and acknowledge missing information. The Phase 2 system meets these requirements partially. Retrieval accuracy is strong (90% relevant in top-5), and citation format compliance is high (95%). The structured citations enhancement successfully implements dual-layer attribution: inline (source_id, chunk_id) pairs enable granular claim verification, while the manifest-derived References section provides academic-standard bibliographic information. However, groundedness falls short of the "accurately backed" standard due to hallucination and weak claims—the automated metric shows only 42% of answer sentences have direct textual support in retrieved chunks, and qualitative analysis reveals fabricated statistics and table references. The system does acknowledge missing evidence in edge cases but inconsistently and often hedges rather than providing definitive absence statements.

Phase 1 prompt evaluation (Prompts A, B, C) showed that explicit citation requirements and "say when unsupported" instructions improved groundedness. The Phase 2 system prompt incorporates these learnings, yet groundedness remains moderate. This suggests prompt engineering alone is insufficient with smaller local LLMs—model capability (instruction-following, hallucination mitigation) is a binding constraint. The addition of automated evaluation metrics in Phase 2 enables quantitative tracking of this gap across system iterations, transforming a qualitative observation into a measurable optimization target.

Phase 1 identified key failure modes (fabricated citations, generic summaries, ignoring absence) that persist in Phase 2, indicating that pipeline architecture (chunking, retrieval) addresses some issues (generic summaries improved) but not others (fabrication, absence handling). The evidence strength scoring feature partially addresses confidence calibration by exposing retrieval quality to users, though it does not prevent the LLM from hallucinating when evidence is weak. Addressing remaining failures likely requires model upgrades or additional safeguards (e.g., post-generation fact-checking, constrained decoding).

### 6.2 Recommendations for Future Enhancement

Several enhancements could improve system performance beyond the Phase 2 baseline. First, implement hybrid retrieval combining dense embeddings with BM25 keyword matching to address semantic mismatch and improve edge-case coverage. This would help queries with specific terminology (e.g., "promotion," "TFP") that are underrepresented in embedding space. The current dense-only retrieval achieves 90% top-5 precision but struggles with rare terms and acronyms.

Second, add reranking after initial retrieval to improve source diversity and relevance. A cross-encoder model or LLM-based reranker could filter and reorder the top-k candidates, balancing semantic similarity with source variety. This would particularly benefit synthesis queries requiring multiple sources, where the current system exhibits source imbalance (e.g., 3-4 chunks from SRC012/SRC013 crowding out other relevant sources).

Third, upgrade the generation model. Replacing gemma3:4b with a larger, more capable local model (e.g., Llama-3.1-8B-Instruct, Mistral-7B-Instruct) or using a quality API model (GPT-4, Claude) would likely reduce hallucination and improve instruction-following. If cost/privacy constraints require local deployment, running a quantized 8B or 13B model with stronger grounding capabilities would be a manageable upgrade. This is the highest-impact improvement given that generation quality is the primary bottleneck.

Fourth, implement structured output constraints. Rather than free-form generation, guide the LLM to produce JSON with {answer, citations, confidence, missing_info} fields. This would enforce citation granularity, enable explicit confidence signals, and separate answer content from references, improving both usability and automated evaluation. This could build on the existing structured citations and evidence strength features.

Fifth, add a post-generation fact-checking module that compares generated claims against retrieved chunks, flagging unsupported statements before returning to the user. This could use semantic similarity or entailment models to verify each claim and insert "[unsupported]" markers or remove fabricated content. This would complement the existing automated groundedness metric by providing real-time guardrails.

Sixth, expand the corpus to 30–50 sources to improve coverage and reduce "corpus lacks evidence" cases. Prioritize sources with tables, structured data, and diverse perspectives (e.g., sectoral studies, international comparisons, dissenting views) to support synthesis queries and reduce edge-case failures.

Seventh, enhance the automated evaluation metrics. The current sentence-matching and keyword-overlap approaches provide a scalable baseline but have known limitations (no paraphrase detection, no semantic entailment checking). Implementing LLM-as-judge scoring (using GPT-4 or Claude to rate groundedness/relevance) or semantic entailment models (e.g., NLI-based verification) would provide richer quality assessment. Additionally, tracking retrieval precision/recall against a gold-standard query-passage mapping would enable more targeted retrieval improvements.

### 6.3 Implications for Research Assistance Use Cases

The current system is suitable for exploratory research support with human-in-the-loop verification. Users can quickly identify relevant sources and get directional answers, but should not trust specific numbers or citations without manual confirmation. The system excels at "which sources discuss X?" queries and provides a useful starting point for synthesis, but requires expert oversight for factual accuracy.

For deployment in academic or policy settings, the system would need significant robustness improvements. Hallucination of statistics is disqualifying in evidence-driven decision contexts. Adding safeguards (structured output, fact-checking, higher-quality models) and expanding evaluation to include adversarial queries (intentionally misleading, requesting non-existent info) would be necessary.

The system demonstrates that RAG architectures can effectively organize and surface research evidence at scale, bridging the gap between keyword search (low recall for complex queries) and full-text reading (time-intensive). However, the "augmented" generation step remains a reliability bottleneck with current open-weight models, suggesting that RAG for high-stakes domains may need to prioritize retrieval quality and present ranked passages rather than generating synthetic answers.

---

## 7. Conclusion

This Phase 2 evaluation assessed a complete RAG pipeline for remote work productivity research using a 15-source corpus and a 20-query test set. The system achieves strong retrieval performance (90% top-5 relevance), disciplined citation format (95% compliance), and appropriate caution in several edge cases. Phase 2 enhancements successfully implement three key features: automated evaluation metrics (groundedness and answer relevance computed via sentence-matching and keyword-overlap algorithms), structured citations (inline source references combined with manifest-derived bibliographic lists), and evidence strength scoring (High/Medium/Low confidence labels based on retrieval similarity). These enhancements provide transparency, scalability, and academic compliance beyond the baseline RAG architecture.

Automated metrics reveal moderate groundedness (42% sentence-level overlap) and high answer relevance (87% keyword overlap). The groundedness metric uses conservative sentence-matching that captures direct quotes but assigns zero credit to paraphrased content, providing a lower bound on factual grounding. Qualitative analysis confirms substantial generation quality issues: hallucination of specific numbers, table references, and citations; verbose output with tangential discussion; and inconsistent instruction-following. Answer relevance is high but limited by the metric's inability to assess factual correctness or completeness.

The primary bottleneck is generation quality with the local LLM (gemma3:4b), which exhibits hallucination of specifics, inconsistent instruction-following, and verbose output. Retrieval limitations (source imbalance, semantic mismatch) also reduce performance for synthesis and edge-case queries. The evidence strength scoring feature provides useful confidence calibration but cannot prevent LLM fabrication when evidence is weak or ambiguous.

Recommended future enhancements include hybrid retrieval, reranking for source diversity, model upgrades (larger local LLMs or API models), structured output constraints building on existing citation features, post-generation fact-checking, expanded corpus coverage, and richer evaluation metrics (LLM-as-judge, semantic entailment). The most impactful improvement would be upgrading the generation model to reduce hallucination and improve instruction-following.

The system demonstrates the feasibility of local, privacy-preserving RAG for research support with automated quality monitoring, but highlights that groundedness and robustness require continued investment in model quality, evaluation rigor, and architectural safeguards. The evaluation framework (test queries, automated metrics, structured logging) provides a foundation for iterative improvement and benchmark tracking in future phases. The Phase 2 enhancements establish a strong technical baseline for systematic RAG development with transparent quality assessment.

---

## Appendix: Evaluation Summary Table

| Query ID | Type | Groundedness | Relevance | Top Chunk Relevant? | Notes |
|----------|------|--------------|-----------|---------------------|-------|
| Q01 | Direct | 0.52 | 0.83 | No | Fabricated 60-min stat; related chunks retrieved |
| Q02 | Direct | 0.42 | 0.87 | Yes | Correct source, hallucinated % reduction |
| Q03 | Direct | 0.44 | 0.92 | Yes | Definition missing, variables generic |
| Q04 | Direct | 0.44 | 0.79 | Yes | Correct source, fabricated BLS citation |
| Q05 | Direct | 0.44 | 0.75 | Yes | Fabricated %, not from Bartik |
| Q06 | Direct | 0.29 | 0.92 | Yes | Correct %, excessive tangential content |
| Q07 | Direct | 0.44 | 0.85 | Yes | Fabricated table references (A3, A5) |
| Q08 | Direct | 0.41 | 0.86 | No | Retrieval failure, hallucinated DB list |
| Q09 | Direct | 0.38 | 1.00 | Partial | Discussed relationship, weak sample size support |
| Q10 | Direct | 0.39 | 0.93 | - | (Not evaluated in provided run) |
| Q11 | Synthesis | 0.44 | 0.89 | Yes | Comparison present, limitations weak |
| Q12 | Synthesis | 0.42 | 0.84 | Yes | Mechanisms integrated, some unsupported |
| Q13 | Synthesis | - | - | - | (Not evaluated in provided run) |
| Q14 | Synthesis | 0.41 | 0.82 | Partial | Source imbalance, limited SRC008/015 |
| Q15 | Synthesis | - | - | - | (Not evaluated in provided run) |
| Q16 | Edge | 0.38 | 0.88 | No | Correctly stated not found, then speculated |
| Q17 | Edge | 0.36 | 0.84 | Partial | Classification errors, weak justification |
| Q18 | Edge | 0.39 | 0.86 | Yes | Correct sources, overstated consensus |
| Q19 | Edge | 0.40 | 0.85 | Partial | Acknowledged gaps, added unsupported discussion |
| Q20 | Edge | - | - | - | (Not evaluated in provided run) |

**Mean (evaluated queries):** Groundedness 0.42, Relevance 0.87  
**Retrieval Top-5 Precision:** 18/20 (90%)  
**Citation Format Compliance:** 19/20 (95%)

---

## References

1. Aksoy et al. (2023). Time Savings When Working from Home. NBER Working Paper w30866.
2. Bloom, Han & Liang (2024). Hybrid Working from Home Improves Retention without Damaging Performance. Nature.
3. Barrero, Bloom & Davis (2023). The Evolution of Work from Home. Journal of Economic Perspectives.
4. Gibbs, Mengel & Siemroth (2023). Work from Home and Productivity. Journal of Political Economy Microeconomics.
5. Phase 1 Individual Project Assignment (CMU Heinz AIM, 2026).
6. Research Portal Data Manifest (data_manifest.csv).
7. Evaluation Query Set (queries.jsonl, 20 queries).
8. Evaluation Run Results (outputs/eval_runs/eval_run10.jsonl).
